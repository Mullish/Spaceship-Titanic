{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Космический корабль Титаник\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "p_JViaUaJ13O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Boostings"
      ],
      "metadata": {
        "id": "buGVHJj3c9cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "pip install catboost"
      ],
      "metadata": {
        "id": "r8JDPMxWdHf7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_missing_values(df):\n",
        "\n",
        "    missing_values = df.isnull().sum()\n",
        "    if missing_values.sum() > 0:\n",
        "        print(\"Обнаружены пропущенные значения:\")\n",
        "        print(missing_values[missing_values > 0])\n",
        "        return True\n",
        "    else:\n",
        "        print(\"Пропущенные значения не обнаружены.\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "f8pAv89zdT3g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, model_name=\"Model\"):\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.ylabel('Actual Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yM-nsETadWBe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_roc_curve(y_true, y_proba, model_name=\"Model\"):\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'Receiver Operating Characteristic (ROC) - {model_name}')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "JUNvAfO3dXdL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import xgboost as xgb\n",
        "    from xgboost import XGBClassifier\n",
        "    XGB_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"XGBoost is not installed.  Install it with: pip install xgboost\")\n",
        "    XGB_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    from lightgbm import LGBMClassifier\n",
        "    LGBM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"LightGBM is not installed. Install it with: pip install lightgbm\")\n",
        "    LGBM_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "    from catboost import CatBoostClassifier\n",
        "    CATBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"CatBoost is not installed. Install it with: pip install catboost\")\n",
        "    CATBOOST_AVAILABLE = False"
      ],
      "metadata": {
        "id": "uhQ7BMzXD59a"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_forest_pipeline_with_imputation(train_data, target_column, imputation_strategy='median', knn_neighbors=5, random_state=42):\n",
        "\n",
        "    X = train_data.drop(target_column, axis=1)\n",
        "    y = train_data[target_column].astype(int)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "    numerical_features = X_train.select_dtypes(include=['number']).columns\n",
        "    categorical_features = X_train.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "    if imputation_strategy == 'mean' or imputation_strategy == 'median':\n",
        "        numerical_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy=imputation_strategy)),\n",
        "            ('scaler', StandardScaler())\n",
        "        ])\n",
        "    elif imputation_strategy == 'knn':\n",
        "        if len(numerical_features) == 0:\n",
        "            imputation_strategy = 'median'\n",
        "            print(\"Warning: No numerical features for KNN imputation. Switching to median imputation.\")\n",
        "            numerical_transformer = Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy=imputation_strategy)),\n",
        "                ('scaler', StandardScaler())\n",
        "            ])\n",
        "        else:\n",
        "            numerical_transformer = Pipeline(steps=[\n",
        "                ('imputer', KNNImputer(n_neighbors=knn_neighbors)),\n",
        "                ('scaler', StandardScaler())\n",
        "            ])\n",
        "    else:\n",
        "        raise ValueError(\"Invalid imputation strategy. Must be 'mean', 'median', or 'knn'.\")\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ])\n",
        "\n",
        "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', RandomForestClassifier(random_state=random_state))])\n",
        "\n",
        "    param_grid = {\n",
        "        'classifier__n_estimators': [50, 100, 200],\n",
        "        'classifier__max_depth': [None, 10, 20],\n",
        "        'classifier__min_samples_split': [2, 5, 10],\n",
        "        'classifier__min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "\n",
        "    if np.isnan(X_train_processed.toarray() if hasattr(X_train_processed, 'toarray') else X_train_processed).any():\n",
        "        raise ValueError(\"NaN values present in X_train_processed after preprocessing.\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        grid_search.fit(X_train, y_train)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during GridSearchCV fitting: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    return grid_search.best_estimator_, grid_search.best_params_"
      ],
      "metadata": {
        "id": "hqslHPQ4EASl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def boosting_pipeline_with_imputation(train_data, target_column, boosting_type='gradient_boosting', imputation_strategy='median', knn_neighbors=5, random_state=42):\n",
        "\n",
        "    X = train_data.drop(target_column, axis=1)\n",
        "    y = train_data[target_column].astype(int)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "    numerical_features = X_train.select_dtypes(include=['number']).columns\n",
        "    categorical_features = X_train.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "    if imputation_strategy == 'mean' or imputation_strategy == 'median':\n",
        "        numerical_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy=imputation_strategy)),\n",
        "            ('scaler', StandardScaler())\n",
        "        ])\n",
        "    elif imputation_strategy == 'knn':\n",
        "        if len(numerical_features) == 0:\n",
        "            imputation_strategy = 'median'\n",
        "            print(\"Warning: No numerical features for KNN imputation. Switching to median imputation.\")\n",
        "            numerical_transformer = Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy=imputation_strategy)),\n",
        "                ('scaler', StandardScaler())\n",
        "            ])\n",
        "        else:\n",
        "            numerical_transformer = Pipeline(steps=[\n",
        "                ('imputer', KNNImputer(n_neighbors=knn_neighbors)),\n",
        "                ('scaler', StandardScaler())\n",
        "            ])\n",
        "    else:\n",
        "        raise ValueError(\"Invalid imputation strategy. Must be 'mean', 'median', or 'knn'.\")\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ])\n",
        "\n",
        "    if boosting_type == 'gradient_boosting':\n",
        "        classifier = GradientBoostingClassifier(random_state=random_state)\n",
        "        param_grid = {\n",
        "            'classifier__n_estimators': [50, 100, 200],\n",
        "            'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
        "            'classifier__max_depth': [3, 5, 7],\n",
        "            'classifier__min_samples_split': [2, 4],\n",
        "            'classifier__min_samples_leaf': [1, 2]\n",
        "        }\n",
        "    elif boosting_type == 'adaboost':\n",
        "        classifier = AdaBoostClassifier(random_state=random_state)\n",
        "        param_grid = {\n",
        "            'classifier__n_estimators': [50, 100, 200],\n",
        "            'classifier__learning_rate': [0.01, 0.1, 0.2]\n",
        "        }\n",
        "    elif boosting_type == 'xgboost':\n",
        "        if not XGB_AVAILABLE:\n",
        "            print(\"XGBoost is not available. Skipping.\")\n",
        "            return None, None\n",
        "        classifier = XGBClassifier(random_state=random_state, use_label_encoder=False, eval_metric='logloss')\n",
        "        param_grid = {\n",
        "            'classifier__n_estimators': [50, 100, 200],\n",
        "            'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
        "            'classifier__max_depth': [3, 5, 7],\n",
        "            'classifier__subsample': [0.8, 1.0],\n",
        "            'classifier__colsample_bytree': [0.8, 1.0]\n",
        "        }\n",
        "    elif boosting_type == 'lightgbm':\n",
        "        if not LGBM_AVAILABLE:\n",
        "            print(\"LightGBM is not available. Skipping.\")\n",
        "            return None, None\n",
        "        classifier = LGBMClassifier(random_state=random_state)\n",
        "        param_grid = {\n",
        "            'classifier__n_estimators': [50, 100, 200],\n",
        "            'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
        "            'classifier__max_depth': [3, 5, 7],\n",
        "            'classifier__num_leaves': [20, 31, 40],\n",
        "            'classifier__reg_alpha': [0.0, 0.1],    # L1 regularization\n",
        "            'classifier__reg_lambda': [0.0, 0.1]   # L2 regularization\n",
        "        }\n",
        "    elif boosting_type == 'catboost':\n",
        "        if not CATBOOST_AVAILABLE:\n",
        "            print(\"CatBoost is not available. Skipping.\")\n",
        "            return None, None\n",
        "\n",
        "        classifier = CatBoostClassifier(random_state=random_state, verbose=0)  # verbose=0 to suppress training output\n",
        "        param_grid = {\n",
        "            'classifier__iterations': [50, 100, 200],\n",
        "            'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
        "            'classifier__depth': [3, 5, 7],\n",
        "            'classifier__l2_leaf_reg': [1, 3, 5]  # L2 regularization\n",
        "        }\n",
        "    elif boosting_type == 'logistic_regression':\n",
        "        classifier = LogisticRegression(random_state=random_state, solver='liblinear')\n",
        "        param_grid = {\n",
        "            'classifier__penalty': ['l1', 'l2'],\n",
        "            'classifier__C': [0.1, 1.0, 10.0]\n",
        "        }\n",
        "    else:\n",
        "        raise ValueError(\"Invalid boosting type. Must be 'gradient_boosting', 'adaboost', 'xgboost', 'lightgbm', 'catboost', or 'logistic_regression'.\")\n",
        "\n",
        "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', classifier)])\n",
        "\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "\n",
        "    if np.isnan(X_train_processed.toarray() if hasattr(X_train_processed, 'toarray') else X_train_processed).any():\n",
        "        raise ValueError(\"NaN values present in X_train_processed after preprocessing.\")\n",
        "\n",
        "    try:\n",
        "        grid_search.fit(X_train, y_train)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during GridSearchCV fitting: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    return grid_search.best_estimator_, grid_search.best_params_"
      ],
      "metadata": {
        "id": "KVnKPolEEK8V"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    train_data = pd.read_csv('train.csv')\n",
        "    test_data = pd.read_csv('test.csv')\n",
        "\n",
        "    train_data['Transported'] = train_data['Transported'].astype(str).map({'True': 1, 'False': 0})\n",
        "    target_column = 'Transported'\n",
        "\n",
        "    print(\"Checking missing values in training data:\")\n",
        "    has_missing_train = check_missing_values(train_data)\n",
        "    print(\"\\nChecking missing values in test data:\")\n",
        "    has_missing_test = check_missing_values(test_data)\n",
        "\n",
        "    imputation_strategies = ['mean', 'median', 'knn']\n",
        "    boosting_types = ['gradient_boosting', 'adaboost', 'xgboost', 'lightgbm', 'catboost']\n",
        "\n",
        "    all_models = []\n",
        "\n",
        "    for imputation_strategy in imputation_strategies:\n",
        "        print(f\"\\nTraining Random Forest with imputation strategy: {imputation_strategy}\")\n",
        "        pipeline, params = random_forest_pipeline_with_imputation(\n",
        "            train_data.copy(), target_column, imputation_strategy=imputation_strategy\n",
        "        )\n",
        "        if pipeline:\n",
        "            print(f\"Best parameters for Random Forest ({imputation_strategy}): {params}\")\n",
        "            all_models.append((f\"RandomForest_{imputation_strategy}\", pipeline))\n",
        "\n",
        "        for boosting_type in boosting_types:\n",
        "            print(f\"\\nTraining {boosting_type} with imputation strategy: {imputation_strategy}\")\n",
        "            pipeline, params = boosting_pipeline_with_imputation(\n",
        "                train_data.copy(), target_column, boosting_type=boosting_type, imputation_strategy=imputation_strategy\n",
        "            )\n",
        "            if pipeline:\n",
        "                print(f\"Best parameters for {boosting_type} ({imputation_strategy}): {params}\")\n",
        "                all_models.append((f\"{boosting_type}_{imputation_strategy}\", pipeline))\n",
        "\n",
        "\n",
        "    print(\"\\nTraining complete. Trained the following models:\")\n",
        "    for model_name, model in all_models:\n",
        "        print(f\"- {model_name}\")\n",
        "\n",
        "    if all_models:\n",
        "        X = train_data.drop(target_column, axis=1)\n",
        "        y = train_data[target_column].astype(int)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        print(\"\\nEvaluating models on validation set:\")\n",
        "        for model_name, model in all_models:\n",
        "            try:\n",
        "                y_pred = model.predict(X_val)\n",
        "                accuracy = accuracy_score(y_val, y_pred)\n",
        "                print(f\"{model_name} accuracy: {accuracy:.4f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error during prediction with {model_name}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H72Q-8LvEQuY",
        "outputId": "5fc1f82c-b32c-4134-b73d-f88a18601136"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking missing values in training data:\n",
            "Обнаружены пропущенные значения:\n",
            "HomePlanet      201\n",
            "CryoSleep       217\n",
            "Cabin           199\n",
            "Destination     182\n",
            "Age             179\n",
            "VIP             203\n",
            "RoomService     181\n",
            "FoodCourt       183\n",
            "ShoppingMall    208\n",
            "Spa             183\n",
            "VRDeck          188\n",
            "Name            200\n",
            "dtype: int64\n",
            "\n",
            "Checking missing values in test data:\n",
            "Обнаружены пропущенные значения:\n",
            "HomePlanet       87\n",
            "CryoSleep        93\n",
            "Cabin           100\n",
            "Destination      92\n",
            "Age              91\n",
            "VIP              93\n",
            "RoomService      82\n",
            "FoodCourt       106\n",
            "ShoppingMall     98\n",
            "Spa             101\n",
            "VRDeck           80\n",
            "Name             94\n",
            "dtype: int64\n",
            "\n",
            "Training Random Forest with imputation strategy: mean\n",
            "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
            "Best parameters for Random Forest (mean): {'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 200}\n",
            "\n",
            "Training gradient_boosting with imputation strategy: mean\n",
            "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
            "Best parameters for gradient_boosting (mean): {'classifier__learning_rate': 0.2, 'classifier__max_depth': 5, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 50}\n",
            "\n",
            "Training adaboost with imputation strategy: mean\n",
            "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
            "Best parameters for adaboost (mean): {'classifier__learning_rate': 0.2, 'classifier__n_estimators': 200}\n",
            "\n",
            "Training xgboost with imputation strategy: mean\n",
            "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
            "Best parameters for xgboost (mean): {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.2, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__subsample': 1.0}\n",
            "\n",
            "Training lightgbm with imputation strategy: mean\n",
            "Fitting 3 folds for each of 324 candidates, totalling 972 fits\n",
            "[LightGBM] [Info] Number of positive: 3500, number of negative: 3454\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000937 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1378\n",
            "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 18\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503307 -> initscore=0.013230\n",
            "[LightGBM] [Info] Start training from score 0.013230\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Best parameters for lightgbm (mean): {'classifier__learning_rate': 0.1, 'classifier__max_depth': 7, 'classifier__n_estimators': 50, 'classifier__num_leaves': 40, 'classifier__reg_alpha': 0.1, 'classifier__reg_lambda': 0.0}\n",
            "\n",
            "Training catboost with imputation strategy: mean\n",
            "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
            "Best parameters for catboost (mean): {'classifier__depth': 5, 'classifier__iterations': 50, 'classifier__l2_leaf_reg': 1, 'classifier__learning_rate': 0.2}\n",
            "\n",
            "Training Random Forest with imputation strategy: median\n",
            "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
            "Best parameters for Random Forest (median): {'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100}\n",
            "\n",
            "Training gradient_boosting with imputation strategy: median\n",
            "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
            "Best parameters for gradient_boosting (median): {'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 4, 'classifier__n_estimators': 200}\n",
            "\n",
            "Training adaboost with imputation strategy: median\n",
            "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
            "Best parameters for adaboost (median): {'classifier__learning_rate': 0.2, 'classifier__n_estimators': 200}\n",
            "\n",
            "Training xgboost with imputation strategy: median\n",
            "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
            "Best parameters for xgboost (median): {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.1, 'classifier__max_depth': 7, 'classifier__n_estimators': 50, 'classifier__subsample': 1.0}\n",
            "\n",
            "Training lightgbm with imputation strategy: median\n",
            "Fitting 3 folds for each of 324 candidates, totalling 972 fits\n",
            "[LightGBM] [Info] Number of positive: 3500, number of negative: 3454\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000843 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1378\n",
            "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 18\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503307 -> initscore=0.013230\n",
            "[LightGBM] [Info] Start training from score 0.013230\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Best parameters for lightgbm (median): {'classifier__learning_rate': 0.1, 'classifier__max_depth': 7, 'classifier__n_estimators': 50, 'classifier__num_leaves': 40, 'classifier__reg_alpha': 0.1, 'classifier__reg_lambda': 0.0}\n",
            "\n",
            "Training catboost with imputation strategy: median\n",
            "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
            "Best parameters for catboost (median): {'classifier__depth': 7, 'classifier__iterations': 100, 'classifier__l2_leaf_reg': 1, 'classifier__learning_rate': 0.1}\n",
            "\n",
            "Training Random Forest with imputation strategy: knn\n",
            "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
            "Best parameters for Random Forest (knn): {'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 200}\n",
            "\n",
            "Training gradient_boosting with imputation strategy: knn\n",
            "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
            "Best parameters for gradient_boosting (knn): {'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 200}\n",
            "\n",
            "Training adaboost with imputation strategy: knn\n",
            "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
            "Best parameters for adaboost (knn): {'classifier__learning_rate': 0.2, 'classifier__n_estimators': 200}\n",
            "\n",
            "Training xgboost with imputation strategy: knn\n",
            "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
            "Best parameters for xgboost (knn): {'classifier__colsample_bytree': 1.0, 'classifier__learning_rate': 0.2, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__subsample': 0.8}\n",
            "\n",
            "Training lightgbm with imputation strategy: knn\n",
            "Fitting 3 folds for each of 324 candidates, totalling 972 fits\n",
            "[LightGBM] [Info] Number of positive: 3500, number of negative: 3454\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001048 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1393\n",
            "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 18\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503307 -> initscore=0.013230\n",
            "[LightGBM] [Info] Start training from score 0.013230\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Best parameters for lightgbm (knn): {'classifier__learning_rate': 0.1, 'classifier__max_depth': 7, 'classifier__n_estimators': 50, 'classifier__num_leaves': 40, 'classifier__reg_alpha': 0.0, 'classifier__reg_lambda': 0.1}\n",
            "\n",
            "Training catboost with imputation strategy: knn\n",
            "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
            "Best parameters for catboost (knn): {'classifier__depth': 7, 'classifier__iterations': 200, 'classifier__l2_leaf_reg': 5, 'classifier__learning_rate': 0.2}\n",
            "\n",
            "Training complete. Trained the following models:\n",
            "- RandomForest_mean\n",
            "- gradient_boosting_mean\n",
            "- adaboost_mean\n",
            "- xgboost_mean\n",
            "- lightgbm_mean\n",
            "- catboost_mean\n",
            "- RandomForest_median\n",
            "- gradient_boosting_median\n",
            "- adaboost_median\n",
            "- xgboost_median\n",
            "- lightgbm_median\n",
            "- catboost_median\n",
            "- RandomForest_knn\n",
            "- gradient_boosting_knn\n",
            "- adaboost_knn\n",
            "- xgboost_knn\n",
            "- lightgbm_knn\n",
            "- catboost_knn\n",
            "\n",
            "Evaluating models on validation set:\n",
            "RandomForest_mean accuracy: 0.7890\n",
            "gradient_boosting_mean accuracy: 0.7867\n",
            "adaboost_mean accuracy: 0.7499\n",
            "xgboost_mean accuracy: 0.7855\n",
            "lightgbm_mean accuracy: 0.7849\n",
            "catboost_mean accuracy: 0.7815\n",
            "RandomForest_median accuracy: 0.7815\n",
            "gradient_boosting_median accuracy: 0.7901\n",
            "adaboost_median accuracy: 0.7481\n",
            "xgboost_median accuracy: 0.7878\n",
            "lightgbm_median accuracy: 0.7861\n",
            "catboost_median accuracy: 0.7798\n",
            "RandomForest_knn accuracy: 0.7838\n",
            "gradient_boosting_knn accuracy: 0.7878\n",
            "adaboost_knn accuracy: 0.7527\n",
            "xgboost_knn accuracy: 0.7815\n",
            "lightgbm_knn accuracy: 0.7895\n",
            "catboost_knn accuracy: 0.7838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best model on validation set is gradient_boosting_median with accuracy 0.7901"
      ],
      "metadata": {
        "id": "Oej-UKNcQkWK"
      }
    }
  ]
}